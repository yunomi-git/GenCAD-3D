# Adapted from Alam et al, https://github.com/ferdous-alam/GenCAD

import math
from pathlib import Path
from functools import partial
from collections import namedtuple
import os
from tqdm.auto import tqdm
import numpy as np
import h5py
import torch
import matplotlib.pyplot as plt

from torch import nn
import torch.nn.functional as F
from torch.cuda.amp import autocast, GradScaler
from torch.optim import Adam
from torch.utils.data import Dataset, DataLoader

from einops import reduce
from ema_pytorch import EMA
from lightning.fabric import Fabric
from sklearn.decomposition import PCA

from tensorboardX import SummaryWriter


ModelPrediction =  namedtuple('ModelPrediction', ['pred_noise', 'pred_x_start'])


class Dataset1D(Dataset):
    def __init__(self, cad_tensor, image_tensor):
        super().__init__()
        self.cad_tensor = cad_tensor.clone()
        self.image_tensor = image_tensor.clone()
        
    def __len__(self):
        return len(self.cad_tensor)

    def __getitem__(self, idx):
        return self.cad_tensor[idx].clone(),  self.image_tensor[idx].clone()
    

def exists(x):
    return x is not None

def default(val, d):
    if exists(val):
        return val
    return d() if callable(d) else d

def identity(t, *args, **kwargs):
    return t

def cycle(dl):
    while True:
        for data in dl:
            yield data

def has_int_squareroot(num):
    return (math.sqrt(num) ** 2) == num

def num_to_groups(num, divisor):
    groups = num // divisor
    remainder = num % divisor
    arr = [divisor] * groups
    if remainder > 0:
        arr.append(remainder)
    return arr

def convert_image_to_fn(img_type, image):
    if image.mode != img_type:
        return image.convert(img_type)
    return image

# normalization functions

def normalize_to_neg_one_to_one(img):
    return img * 2 - 1

def unnormalize_to_zero_to_one(t):
    return (t + 1) * 0.5


# gaussian diffusion trainer class

def extract(a, t, x_shape):
    b, *_ = t.shape
    out = a.gather(-1, t)
    return out.reshape(b, *((1,) * (len(x_shape) - 1)))

def linear_beta_schedule(timesteps):
    scale = 1000 / timesteps
    beta_start = scale * 0.0001
    beta_end = scale * 0.02
    return torch.linspace(beta_start, beta_end, timesteps, dtype = torch.float64)

def cosine_beta_schedule(timesteps, s = 0.008):
    """
    cosine schedule
    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ
    """
    steps = timesteps + 1
    x = torch.linspace(0, timesteps, steps, dtype = torch.float64)
    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2
    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
    return torch.clip(betas, 0, 0.999)



class GaussianDiffusion1D(nn.Module):
    def __init__(
        self,
        model,
        *,
        z_dim,
        timesteps = 1000,
        sampling_timesteps = None,
        objective = 'pred_noise',
        beta_schedule = 'cosine',
        ddim_sampling_eta = 0.,
        auto_normalize = True
    ):
        super().__init__()
        self.model = model

        self.objective = objective

        self.z_dim = z_dim

        assert objective in {'pred_noise', 'pred_x0', 'pred_v'}, 'objective must be either pred_noise (predict noise) or pred_x0 (predict image start) or pred_v (predict v [v-parameterization as defined in appendix D of progressive distillation paper, used in imagen-video successfully])'

        if beta_schedule == 'linear':
            betas = linear_beta_schedule(timesteps)
        elif beta_schedule == 'cosine':
            betas = cosine_beta_schedule(timesteps)
        else:
            raise ValueError(f'unknown beta schedule {beta_schedule}')

        alphas = 1. - betas
        alphas_cumprod = torch.cumprod(alphas, dim=0)
        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)

        timesteps, = betas.shape
        self.num_timesteps = int(timesteps)

        # sampling related parameters

        self.sampling_timesteps = default(sampling_timesteps, timesteps) # default num sampling timesteps to number of timesteps at training

        assert self.sampling_timesteps <= timesteps
        self.is_ddim_sampling = self.sampling_timesteps < timesteps
        self.ddim_sampling_eta = ddim_sampling_eta

        # helper function to register buffer from float64 to float32

        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))

        register_buffer('betas', betas)
        register_buffer('alphas_cumprod', alphas_cumprod)
        register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)

        # calculations for diffusion q(x_t | x_{t-1}) and others

        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))
        register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))
        register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))
        register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))
        register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))

        # calculations for posterior q(x_{t-1} | x_t, x_0)

        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)

        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)

        register_buffer('posterior_variance', posterior_variance)

        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain

        register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min =1e-20)))
        register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))
        register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))

        # calculate loss weight

        snr = alphas_cumprod / (1 - alphas_cumprod)

        if objective == 'pred_noise':
            loss_weight = torch.ones_like(snr)
        elif objective == 'pred_x0':
            loss_weight = snr
        elif objective == 'pred_v':
            loss_weight = snr / (snr + 1)

        register_buffer('loss_weight', loss_weight)

        # whether to autonormalize

        self.normalize = normalize_to_neg_one_to_one if auto_normalize else identity
        self.unnormalize = unnormalize_to_zero_to_one if auto_normalize else identity

    def predict_start_from_noise(self, x_t, t, noise):
        return (
            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -
            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise
        )

    def predict_noise_from_start(self, x_t, t, x0):
        return (
            (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) / \
            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)
        )

    def predict_v(self, x_start, t, noise):
        return (
            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * noise -
            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * x_start
        )

    def predict_start_from_v(self, x_t, t, v):
        return (
            extract(self.sqrt_alphas_cumprod, t, x_t.shape) * x_t -
            extract(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v
        )

    def q_posterior(self, x_start, x_t, t):
        posterior_mean = (
            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +
            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t
        )
        posterior_variance = extract(self.posterior_variance, t, x_t.shape)
        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)
        return posterior_mean, posterior_variance, posterior_log_variance_clipped

    def model_predictions(self, x, t, cond = None, clip_x_start = False, rederive_pred_noise = False):
        model_output = self.model(x, t, cond)
        maybe_clip = partial(torch.clamp, min = -1., max = 1.) if clip_x_start else identity

        if self.objective == 'pred_noise':
            pred_noise = model_output
            x_start = self.predict_start_from_noise(x, t, pred_noise)
            x_start = maybe_clip(x_start)

            if clip_x_start and rederive_pred_noise:
                pred_noise = self.predict_noise_from_start(x, t, x_start)

        elif self.objective == 'pred_x0':
            x_start = model_output
            x_start = maybe_clip(x_start)
            pred_noise = self.predict_noise_from_start(x, t, x_start)

        elif self.objective == 'pred_v':
            v = model_output
            x_start = self.predict_start_from_v(x, t, v)
            x_start = maybe_clip(x_start)
            pred_noise = self.predict_noise_from_start(x, t, x_start)

        return ModelPrediction(pred_noise, x_start)

    def p_mean_variance(self, x, t, cond = None, clip_denoised = True):
        preds = self.model_predictions(x, t, cond)
        x_start = preds.pred_x_start

        if clip_denoised:
            x_start.clamp_(-1., 1.)

        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start = x_start, x_t = x, t = t)
        return model_mean, posterior_variance, posterior_log_variance, x_start

    @torch.no_grad()
    def p_sample(self, x, t: int, cond = None, clip_denoised = True):
        b, *_, device = *x.shape, x.device
        batched_times = torch.full((b,), t, device = x.device, dtype = torch.long)
        model_mean, _, model_log_variance, x_start = self.p_mean_variance(x = x, t = batched_times, cond = cond, clip_denoised = clip_denoised)
        noise = torch.randn_like(x) if t > 0 else 0. # no noise if t == 0
        pred_img = model_mean + (0.5 * model_log_variance).exp() * noise
        return pred_img, x_start

    @torch.no_grad()
    def p_sample_loop(self, val):
        shape = val[:-1]
        cond = val[-1]
        device = self.betas.device

        img = torch.randn(shape, device=device)

        x_start = None

        for t in tqdm(reversed(range(0, self.num_timesteps)), desc = 'sampling loop time step', total = self.num_timesteps):
            img, x_start = self.p_sample(img, t, cond=cond)

        img = self.unnormalize(img)
        return img

    @torch.no_grad()
    def ddim_sample(self, shape, clip_denoised = True):
        batch, device, total_timesteps, sampling_timesteps, eta, objective = shape[0], self.betas.device, self.num_timesteps, self.sampling_timesteps, self.ddim_sampling_eta, self.objective

        times = torch.linspace(-1, total_timesteps - 1, steps=sampling_timesteps + 1)   # [-1, 0, 1, 2, ..., T-1] when sampling_timesteps == total_timesteps
        times = list(reversed(times.int().tolist()))
        time_pairs = list(zip(times[:-1], times[1:])) # [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, -1)]

        img = torch.randn(shape, device = device)

        x_start = None

        for time, time_next in tqdm(time_pairs, desc = 'sampling loop time step'):
            time_cond = torch.full((batch,), time, device=device, dtype=torch.long)
            self_cond = x_start if self.self_condition else None
            pred_noise, x_start, *_ = self.model_predictions(img, time_cond, self_cond, clip_x_start = clip_denoised)

            if time_next < 0:
                img = x_start
                continue

            alpha = self.alphas_cumprod[time]
            alpha_next = self.alphas_cumprod[time_next]

            sigma = eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()
            c = (1 - alpha_next - sigma ** 2).sqrt()

            noise = torch.randn_like(img)

            img = x_start * alpha_next.sqrt() + \
                  c * pred_noise + \
                  sigma * noise

        img = self.unnormalize(img)
        return img

    @torch.no_grad()
    def sample(self, cond=None):
        z_dim = self.z_dim
        batch_size = cond.shape[0]
        sample_fn = self.p_sample_loop if not self.is_ddim_sampling else self.ddim_sample
        return sample_fn((batch_size, z_dim, cond))

    @torch.no_grad()
    def interpolate(self, x1, x2, t = None, lam = 0.5):
        b, *_, device = *x1.shape, x1.device
        t = default(t, self.num_timesteps - 1)

        assert x1.shape == x2.shape

        t_batched = torch.full((b,), t, device = device)
        xt1, xt2 = map(lambda x: self.q_sample(x, t = t_batched), (x1, x2))

        img = (1 - lam) * xt1 + lam * xt2

        x_start = None

        for i in tqdm(reversed(range(0, t)), desc = 'interpolation sample time step', total = t):
            self_cond = x_start if self.self_condition else None
            img, x_start = self.p_sample(img, i, self_cond)

        return img

    @autocast(enabled = False)
    def q_sample(self, x_start, t, noise=None):
        noise = default(noise, lambda: torch.randn_like(x_start))

        return (
            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +
            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise
        )

    def p_losses(self, x_start, t, noise = None, cond = None):
        b, z_dim = x_start.shape
        noise = default(noise, lambda: torch.randn_like(x_start))

        # noise sample

        x = self.q_sample(x_start = x_start, t = t, noise = noise)

        # if doing self-conditioning, 50% of the time, predict x_start from current set of times
        # and condition with unet with that
        # this technique will slow down training by 25%, but seems to lower FID significantly

        x_self_cond = None

        # predict and take gradient step

        model_out = self.model(x, t, cond)

        if self.objective == 'pred_noise':
            target = noise
        elif self.objective == 'pred_x0':
            target = x_start
        elif self.objective == 'pred_v':
            v = self.predict_v(x_start, t, noise)
            target = v
        else:
            raise ValueError(f'unknown objective {self.objective}')

        loss = F.mse_loss(model_out, target, reduction = 'none')
        loss = reduce(loss, 'b ... -> b', 'mean')

        loss = loss * extract(self.loss_weight, t, loss.shape)
        return loss.mean()

    def forward(self, img, *args, **kwargs):
        b, z_dim, device = *img.shape, img.device
        assert z_dim == self.z_dim, f'latent dim must be {self.z_dim}'
        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()

        img = self.normalize(img)
        return self.p_losses(img, t, *args, **kwargs)
    

class Trainer1D(object):
    def __init__(
        self,
        diffusion_model: GaussianDiffusion1D,
        dataset,
        *,
        device=torch.device("cpu"),
        train_batch_size = 16,
        gradient_accumulate_every = 1,
        train_lr = 1e-4,
        train_num_steps = 100000,
        ema_update_every = 10,
        ema_decay = 0.995,
        adam_betas = (0.9, 0.99),
        save_and_sample_every = 1000,
        num_samples = 25,
        results_folder = './results',
        max_grad_norm = 1.,
        gt_data_path='data/cad_embeddings.h5',
        amp=False,
        fabric: Fabric=None,
        num_workers=0
    ):
        super().__init__()

        # Distributed setup
        self.fabric = fabric
        self.distributed = False
        if self.fabric is not None:
            self.distributed = True
        self.device = device # Only if not distributed

        self.gt_data_path = gt_data_path
        with h5py.File(self.gt_data_path, 'r') as f:
            self.gt_data = f["train_zs"][:]
        self.gt_latent = self._get_data(self.gt_data, gt=True)

        # optimizer
        self.opt = Adam(diffusion_model.parameters(), lr = train_lr, betas = adam_betas)

        # model
        self.model = diffusion_model
        self.ema = EMA(diffusion_model, beta=ema_decay, update_every=ema_update_every)

        if self.distributed:
            self.model, self.opt = fabric.setup(self.model, self.opt)
            self.ema = fabric.setup(self.ema)
        else:
            self.model.to(device)
            self.ema.to(self.device)

        # dataset and dataloader
        dl = DataLoader(dataset, batch_size=train_batch_size, shuffle=True, pin_memory=False,
                        num_workers=num_workers, drop_last=True)

        if self.distributed:
            dl = fabric.setup_dataloaders(dl)

        self.dl = cycle(dl)

        # sampling and training hyperparameters
        assert has_int_squareroot(num_samples), 'number of samples must have an integer square root'
        self.num_samples = num_samples
        self.save_and_sample_every = save_and_sample_every

        self.batch_size = train_batch_size
        self.gradient_accumulate_every = gradient_accumulate_every
        self.max_grad_norm = max_grad_norm

        self.train_num_steps = train_num_steps

        # for logging results in a folder periodically
        self.log_dir = os.path.join(results_folder)
        self.results_folder = Path(self.log_dir)
        self.results_folder.mkdir(exist_ok = True)

        self.models_dir = self.log_dir + "models/"
        self.samples_dir = self.log_dir + "samples/"
        Path(self.models_dir).mkdir(exist_ok=True)
        Path(self.samples_dir).mkdir(exist_ok=True)

        self.train_tb = SummaryWriter(os.path.join(self.log_dir, 'train.events'))

        # step counter state
        self.step = 0

        # Mixed Precision Setup
        self.amp = amp
        self.scaler = GradScaler() if self.amp else None




    def _get_data(self, latent_data, gt=False):        
        pca = PCA(n_components=2)
        pca.fit(latent_data)
        latent_reduced = pca.transform(latent_data)

        return latent_reduced

    def save(self, milestone):
        data = {
            'step': self.step,
            'model': self.model.state_dict(),
            'opt': self.opt.state_dict(),
            'ema': self.ema.state_dict()
        }

        torch.save(data, self.models_dir + f'model-{milestone}.pt')

    def fix_nan_batch_norm_checkpoint(self, checkpoint):
        for key in list(checkpoint.keys()):
            if "running_mean" in key:
                if torch.isnan(checkpoint[key]).any():
                    checkpoint[key] = torch.nan_to_num(checkpoint[key], nan=0.0)

            if "running_var" in key:
                if torch.isnan(checkpoint[key]).any():
                    checkpoint[key] = torch.nan_to_num(checkpoint[key], nan=1.0)



    def load(self, milestone):
        device = self.device

        data = torch.load(self.models_dir + f'model-{milestone}.pt', map_location=device)

        self.fix_nan_batch_norm_checkpoint(data['model'])
        self.fix_nan_batch_norm_checkpoint(data['ema'])

        model = self.model
        model.load_state_dict(data['model'])

        self.step = data['step']
        self.opt.load_state_dict(data['opt'])
        self.ema.load_state_dict(data["ema"])

    def _record_loss(self, loss):
        self.train_tb.add_scalar('loss', loss.item(), self.step)

    def run_validation(self, samples):

        latent_diff = self._get_data(samples)

        plt.figure()
        plt.scatter(self.gt_latent[:, 0], self.gt_latent[:, 1], s=0.5, color='gray', alpha=0.25, label='ground truth')
        plt.scatter(latent_diff[:, 0], latent_diff[:, 1], s=0.5, color='blue', alpha=0.75, label='generated')
        plt.legend(fontsize=14)
        plt.xlim(-2.5, 3)
        plt.ylim(-1.5, 2)
        plt.savefig(f'{self.results_folder}/samples.png')

    def sample_from_conditional(self, cloud_emb, as_numpy=True):
        with torch.no_grad():
            sampled = self.ema.ema_model.sample(cond=cloud_emb)
        sampled = torch.nan_to_num(sampled)
        if as_numpy:
            sampled = sampled.cpu().numpy()
            # sampled = np.nan_to_num(sampled)
        return sampled

    def sample(self, num_batch=1):
        # self.ema.update()
        self.ema.ema_model.eval()
        sampled = []
        for i in range(num_batch):
            print(f"Sampling batch {i}")
            batch = next(self.dl)

            cad_emb, image_emb = batch[0].to(self.device), batch[1].to(self.device)

            with torch.no_grad():
                sampled_i = self.ema.ema_model.sample(cond=image_emb)

            # print(sampled_i.size())

            # The sampled here may contain nan
            sampled_i = sampled_i.cpu().numpy()
            # print(sampled.shape)
            sampled_i = np.nan_to_num(sampled_i)
            sampled.append(sampled_i)

        sampled = np.concatenate(sampled)
        return sampled

    def train(self):

        with tqdm(initial = self.step, total = self.train_num_steps) as pbar:
            while self.step < self.train_num_steps:
                total_loss = 0.

                # Accumulate gradients
                for i in range(self.gradient_accumulate_every):
                    has_accumulated = i == self.gradient_accumulate_every - 1
                    # if self.distributed:
                    #     context = self.fabric.no_backward_sync(self.model, enabled=not has_accumulated)
                    # else:
                    #     context = contextlib.nullcontext
                    # with context:
                    batch = next(self.dl)

                    if self.distributed:
                        cad_emb, image_emb = batch[0], batch[1]
                    else:
                        cad_emb, image_emb = batch[0].to(self.device), batch[1].to(self.device)

                    # Loss
                    if self.amp:
                        with autocast():
                            loss = self.model(cad_emb, cond=image_emb)
                            if torch.isnan(loss):
                                continue
                            loss = loss / self.gradient_accumulate_every
                            total_loss += loss.item()

                    # Gradients
                    if self.distributed:
                        self.fabric.backward(self.scaler.scale(loss))
                    else:
                        self.scaler.scale(loss).backward()
                    
                else:
                    loss = self.model(cad_emb, cond=image_emb) / self.gradient_accumulate_every
                    if torch.isnan(loss):
                        print("NAN detected in loss !!!! ")
                        continue
                    total_loss += loss.item()

                    # Gradients
                    if self.distributed:
                        self.fabric.backward(loss)
                    else:
                        loss.backward()

                self._record_loss(loss)

                pbar.set_description(f'loss: {total_loss:.4f}')

                # Gradients
                if self.amp:
                    self.scaler.unscale_(self.opt)

                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
                
                if self.amp:
                    self.scaler.step(self.opt)
                    self.scaler.update()
                else: 
                    self.opt.step()
    
                self.opt.zero_grad()

                # Step
                self.step += 1

                # Validation Step
                if self.step != 0 and self.step % self.save_and_sample_every == 0:
                    self.ema.update()
                    self.ema.ema_model.eval()

                    batch = next(self.dl)

                    if self.distributed:
                        cad_emb, image_emb = batch[0], batch[1]
                    else:
                        cad_emb, image_emb = batch[0].to(self.device), batch[1].to(self.device)

                    # cad_emb, image_emb = batch[0].to(self.device), batch[1].to(self.device)

                    with torch.no_grad():
                        milestone = self.step // self.save_and_sample_every
                        sampled = self.ema.ema_model.sample(cond=image_emb)
                        with h5py.File(self.samples_dir + f'/davinci_samples_{milestone}.h5', 'w') as f:
                            f.create_dataset('zs', data=sampled.cpu().numpy())

                    print(sampled.size())

                    # The sampled here may contain nan
                    sampled = sampled.cpu().numpy()
                    # print(sampled.shape)
                    sampled = np.nan_to_num(sampled)
                    # print(sampled.shape)
                    self.run_validation(sampled)

                    self.save(milestone)

                pbar.update(1)

        print('# # training complete # #')
